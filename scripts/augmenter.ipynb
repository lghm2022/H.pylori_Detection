{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc9b7a8-9b69-4295-bb1f-2396e2c19c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 03:57:49.497279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 03:57:49.497307: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from past.utils import old_div\n",
    "from clodsa.augmentors.augmentorFactory import createAugmentor\n",
    "from clodsa.transformers.transformerFactory import transformerGenerator\n",
    "from clodsa.techniques.techniqueFactory import createTechnique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29587a-d17b-443c-9065-0a3a342ca4a5",
   "metadata": {},
   "source": [
    "# Base Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2d962-7037-4887-9956-398c7a5a2fbc",
   "metadata": {},
   "source": [
    "## Auxiliary function for Flip function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6183bf35-970b-4829-9636-18593a6ab874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxesFromYOLO(imagePath,labelPath):\n",
    "    '''\n",
    "    Function to get an image and its corresponding bounding box\n",
    "    \n",
    "    Recieves:\n",
    "    imagePath: complete path to the image file\n",
    "    labelPath: complete path to the text file\n",
    "    \n",
    "    Returns:\n",
    "    image: the image as a ndarray object\n",
    "    boxes: the bounding boxes as a list of lists\n",
    "    '''\n",
    "    image = cv2.imread(imagePath)\n",
    "    (hI, wI) = image.shape[:2]\n",
    "    lines = [line.rstrip('\\n') for line in open(labelPath)]\n",
    "    boxes = []\n",
    "    if lines != ['']:\n",
    "        for line in lines:\n",
    "            components = line.split(\" \")\n",
    "            category = components[0]\n",
    "            x  = int(float(components[1])*wI - float(components[3])*wI/2)\n",
    "            y = int(float(components[2])*hI - float(components[4])*hI/2)\n",
    "            h = int(float(components[4])*hI)\n",
    "            w = int(float(components[3])*wI)\n",
    "            boxes.append((category, (x, y, w, h)))\n",
    "    return (image,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0768c207-df72-4428-92bd-fccafb6cacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(boxes,img_shape):\n",
    "    '''\n",
    "    Function to convert labels from the pixel scale to the 0 to 1 scale\n",
    "    \n",
    "    Recieves:\n",
    "    boxes: a list of lists containing the bounding boxes from the boxesFromYOLO function\n",
    "    img_shape: the image shape as a tuple or list\n",
    "    \n",
    "    Returns:\n",
    "    norm_boxes: A list of lists of bounding boxes scaled form 0 to 1\n",
    "    '''\n",
    "    norm_boxes = []\n",
    "    for line in boxes:\n",
    "        h = round(int(line[1][3])/img_shape[0],6)\n",
    "        w = round(int(line[1][2])/img_shape[1],6)\n",
    "        y = round((int(line[1][1]) + h*img_shape[0]/2)/img_shape[0],6)\n",
    "        x = round((int(line[1][0]) + w*img_shape[1]/2)/img_shape[1],6)\n",
    "        norm_boxes.append([0,x,y,w,h])\n",
    "    \n",
    "    return norm_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f598463-0858-4e41-85ec-7fb78f2970a0",
   "metadata": {},
   "source": [
    "## Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e63e11-ad07-4de4-a055-a2a57c27ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply horizontal flipping in an image and it's\n",
    "    corresponding bounding box, if it has one. The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    # if the image has an associated label file, apply the clodsa function, else, apply the OpenCV flip \n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "        hFlipGenerator = transformer(hFlip)\n",
    "        hFlipImg,hFlipBoxes = hFlipGenerator.transform(img,boxes)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH_'+save_img, hFlipImg)\n",
    "        norm_boxes = convert_labels(hFlipBoxes,hFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipH_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        hFlipImg = cv2.flip(img, 1)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH_'+save_img, hFlipImg)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f060a31-e401-4adc-a33d-ca4d07929fd9",
   "metadata": {},
   "source": [
    "## Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9bd3d9c-fdb7-4fb9-928c-8af94fc75e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply vertical flipping in an image and it's\n",
    "    corresponding bounding box, if it has one. The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    # if the image has an associated label file, apply the clodsa function, else, apply the OpenCV flip \n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        vFlip = createTechnique(\"flip\",{\"flip\":0})\n",
    "        vFlipGenerator = transformer(vFlip)\n",
    "        vFlipImg,vFlipBoxes = vFlipGenerator.transform(img,boxes)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV_'+save_img, vFlipImg)\n",
    "        norm_boxes = convert_labels(vFlipBoxes,vFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipV_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        vFlipImg = cv2.flip(img, 0)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV_'+save_img, vFlipImg)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947c9fe-1f4d-4ab3-a7e2-2676410f3a56",
   "metadata": {},
   "source": [
    "## Double Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e775db2-2e14-42a4-9395-698e783456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply vertical and horizontal flipping in an image and it's\n",
    "    corresponding bounding box, if it has one. The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    # if the image has an associated label file, apply the clodsa function, else, apply the OpenCV flip \n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        dFlip = createTechnique(\"flip\",{\"flip\":-1})\n",
    "        dFlipGenerator = transformer(dFlip)\n",
    "        dFlipImg,dFlipBoxes = dFlipGenerator.transform(img,boxes)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD_'+save_img, dFlipImg)\n",
    "        norm_boxes = convert_labels(dFlipBoxes,dFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipD_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        dFlipImg = cv2.flip(img, -1)\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD_'+save_img, dFlipImg)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6866d-b079-43a7-b0d7-a38c22023a5d",
   "metadata": {},
   "source": [
    "# Raise Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caaab9c3-c116-4e1a-96f9-fe4ed236a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply a raising of ^0.9 in the hue channel of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    \n",
    "    img = cv2.imread(img_file)\n",
    "    ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "    ##### begin\n",
    "    imageHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "    identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                          for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "    lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    imageHSV = cv2.LUT(imageHSV, lut)\n",
    "    imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "    #### end\n",
    "    \n",
    "    cv2.imwrite(path_to_save_imgs+'/aug_hue_'+save_img, imageRGB)\n",
    "    # if file has a label associated to it, it is coppied and renamed after the augmentation\n",
    "    if in_labels == True:\n",
    "        shutil.copy(path_labels+'/'+txt_file,path_to_save_labels)\n",
    "        os.rename(path_to_save_labels+'/'+txt_file,path_to_save_labels+'/aug_hue_'+txt_file)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f50e0-5a6d-4850-9598-9d627a228560",
   "metadata": {},
   "source": [
    "# Constrast Adujstment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17f5efc-8ec7-48a4-94a1-80ae8e85a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply a contrast adjust to an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    \n",
    "    #-----Reading the image-----------------------------------------------------\n",
    "    img = cv2.imread(img_file)\n",
    "    #-----Converting image to LAB Color model----------------------------------- \n",
    "    lab= cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    #-----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "    #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    #-----Converting image from LAB Color model to RGB model--------------------\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    cv2.imwrite(path_to_save_imgs+'/aug_contrst_'+save_img, final)\n",
    "    # if file has a label associated to it, it is coppied and renamed after the augmentation\n",
    "    if in_labels == True:\n",
    "        shutil.copy(path_labels+'/'+txt_file,path_to_save_labels)\n",
    "        os.rename(path_to_save_labels+'/'+txt_file,path_to_save_labels+'/aug_contrst_'+txt_file)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0f780-5ffb-4e23-a98a-3f8f06df92b0",
   "metadata": {},
   "source": [
    "# Combining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf041417-1a5b-4a7a-9004-75b66ff756f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Horizontal Flip and Raise Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549ce041-3647-42eb-9ecd-e69256abffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply horizontal flip and raise the hue channel by 0.9 in an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "        hFlipGenerator = transformer(hFlip)\n",
    "        hFlipImg,hFlipBoxes = hFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-hue_'+save_img, imageRGB)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(hFlipBoxes,hFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipH-hue_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        hFlipImg = cv2.flip(img, 1)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-hue_'+save_img, imageRGB)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2da4bf-2fbe-4276-a5cc-0a8064720d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vertical Flip and Raise Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6c8c9a-f279-4d3f-806f-8dac3cfe18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply vertical flip and raise the hue channel by 0.9 in an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        vFlip = createTechnique(\"flip\",{\"flip\":0})\n",
    "        vFlipGenerator = transformer(vFlip)\n",
    "        vFlipImg,vFlipBoxes = vFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-hue_'+save_img, imageRGB)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(vFlipBoxes,vFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipV-hue_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        vFlipImg = cv2.flip(img, 0)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-hue_'+save_img, imageRGB)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552d8e0-30ce-4f93-b4cd-154924a1a943",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Double Flip and Raise Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1777e1c-1f88-4885-99dd-40f9a8970f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply horizontal flip, vertical flip and raise the hue channel by 0.9 in an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        dFlip = createTechnique(\"flip\",{\"flip\":-1})\n",
    "        dFlipGenerator = transformer(dFlip)\n",
    "        dFlipImg,dFlipBoxes = dFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-hue_'+save_img, imageRGB)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(dFlipBoxes,dFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipD-hue_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        dFlipImg = cv2.flip(img, -1)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-hue_'+save_img, imageRGB)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e17f0-4e66-4a15-bb60-68e98dc76b74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Horizontal Flip and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c554fdcb-7ed8-446d-82ba-5311c1e27efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply horizontal flip and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "        hFlipGenerator = transformer(hFlip)\n",
    "        hFlipImg,hFlipBoxes = hFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(hFlipBoxes,hFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipH-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        hFlipImg = cv2.flip(img, 1)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce151fc-e771-4236-bf23-5c46d612471e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vertical Flip and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "518495ff-8003-4940-b948-1b40a19279f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply vertical flip and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        vFlip = createTechnique(\"flip\",{\"flip\":0})\n",
    "        vFlipGenerator = transformer(vFlip)\n",
    "        vFlipImg,vFlipBoxes = vFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(vFlipBoxes,vFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipV-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        vFlipImg = cv2.flip(img, 0)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7086ec2-5737-4948-944b-4378e32493e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Double Flip and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449943b5-7930-49b7-a91e-3f1fea103ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply horizontal flip, vertical flip and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        dFlip = createTechnique(\"flip\",{\"flip\":-1})\n",
    "        dFlipGenerator = transformer(dFlip)\n",
    "        dFlipImg,dFlipBoxes = dFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(dFlipBoxes,dFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipD-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        dFlipImg = cv2.flip(img, -1)\n",
    "        \n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c7b91-3ae1-4490-9299-893ee5654dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raise Hue and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dafbb35-e7f7-40cb-baf7-a75696f79699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to raise the hue channel by 0.9 and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    \n",
    "    img = cv2.imread(img_file)\n",
    "    ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "    ##### begin\n",
    "    imageHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "    identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                          for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "    lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    imageHSV = cv2.LUT(imageHSV, lut)\n",
    "    imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "    #### end\n",
    "        \n",
    "    #-----Converting image to LAB Color model----------------------------------- \n",
    "    lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "    #-----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "    #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    #-----Converting image from LAB Color model to RGB model--------------------\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    cv2.imwrite(path_to_save_imgs+'/aug_hue-contrst_'+save_img, final)\n",
    "    if in_labels == True:\n",
    "        shutil.copy(path_labels+'/'+txt_file,path_to_save_labels)\n",
    "        os.rename(path_to_save_labels+'/'+txt_file,path_to_save_labels+'/aug_hue-contrst_'+txt_file)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17861d04-34c3-459b-8552-ec075d6fc5f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Horizontal Flip, Raise Hue and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc327307-1f5d-4486-a914-1df6cc9c8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply an horizontal flip, raise the hue channel by 0.9 and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        hFlip = createTechnique(\"flip\",{\"flip\":1})\n",
    "        hFlipGenerator = transformer(hFlip)\n",
    "        hFlipImg,hFlipBoxes = hFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-hue-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(hFlipBoxes,hFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipH-hue-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        hFlipImg = cv2.flip(img, 1)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(hFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipH-hue-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a5841-3fd0-4599-8beb-c77d5cd25afb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vertical Flip, Raise Hue and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "094c9f3c-00b7-44b5-9735-893e11637eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply an vertical flip, raise the hue channel by 0.9 and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        vFlip = createTechnique(\"flip\",{\"flip\":0})\n",
    "        vFlipGenerator = transformer(vFlip)\n",
    "        vFlipImg,vFlipBoxes = vFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-hue-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(vFlipBoxes,vFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipV-hue-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        vFlipImg = cv2.flip(img, 0)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(vFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipV-hue-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d331a-9e74-4501-8178-a1d720f327b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Double Flip, Raise Hue and Adjust Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f9d408-7e85-4633-a3e9-acbacde9ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels):\n",
    "    '''\n",
    "    Function to apply an horizontal flip, vertical flip, raise the hue channel by 0.9 and adjust the contrast of an image. \n",
    "    The function also writes the files\n",
    "    \n",
    "    Recieves:\n",
    "    img_file: complete path to the image file\n",
    "    txt_file: name of the text file containing the labels for the image file\n",
    "    path_labels: path to te label files\n",
    "    in_labels: boolean variable indicating if the image has or not a label file associated to it\n",
    "    path_to_save_imgs: path were the modified image will be saved\n",
    "    path_to_save_labels: path were the modified label file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    '''\n",
    "    save_img = img_file.split('/')[-1]\n",
    "    if in_labels == True:\n",
    "        img,boxes = boxesFromYOLO(img_file,path_labels + '/' + txt_file)\n",
    "        transformer = transformerGenerator(\"detection\")\n",
    "        dFlip = createTechnique(\"flip\",{\"flip\":-1})\n",
    "        dFlipGenerator = transformer(dFlip)\n",
    "        dFlipImg,dFlipBoxes = dFlipGenerator.transform(img,boxes)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Salvar imagem no diretório de destino\n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-hue-contrst_'+save_img, final)\n",
    "        # Converter label para 0,1 e salvar no diretório destino\n",
    "        norm_boxes = convert_labels(dFlipBoxes,dFlipImg.shape[:2])\n",
    "        with open(path_to_save_labels+'/aug_flipD-hue-contrst_'+txt_file, \"w\") as label_file:\n",
    "            for line in norm_boxes:\n",
    "                line = [str(x) for x in line]\n",
    "                line = ' '.join(line)\n",
    "                line = line + '\\n'\n",
    "                label_file.write(line)\n",
    "    else:\n",
    "        img = cv2.imread(img_file)\n",
    "        dFlipImg = cv2.flip(img, -1)\n",
    "        \n",
    "        ##### Code sample from https://github.com/joheras/CLoDSA/blob/master/clodsa/clodsa/techniques/raiseHueAugmentationTechnique.py\n",
    "        ##### begin\n",
    "        imageHSV = cv2.cvtColor(dFlipImg, cv2.COLOR_BGR2HSV)\n",
    "        identityV = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        identityH = np.array([((old_div(i, 255.0)) ** 0.9) * 255\n",
    "                              for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        identityS = np.arange(256, dtype=np.dtype('uint8'))\n",
    "        lut = np.dstack((identityH, identityS, identityV))\n",
    "\n",
    "        # apply gamma correction using the lookup table\n",
    "        imageHSV = cv2.LUT(imageHSV, lut)\n",
    "        imageRGB = cv2.cvtColor(imageHSV, cv2.COLOR_HSV2BGR)\n",
    "        #### end\n",
    "\n",
    "        #-----Converting image to LAB Color model----------------------------------- \n",
    "        lab= cv2.cvtColor(imageRGB, cv2.COLOR_BGR2LAB)\n",
    "        #-----Splitting the LAB image to different channels-------------------------\n",
    "        l, a, b = cv2.split(lab)\n",
    "        #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        #-----Converting image from LAB Color model to RGB model--------------------\n",
    "        final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        cv2.imwrite(path_to_save_imgs+'/aug_flipD-hue-contrst_'+save_img, final)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9604661-d5b4-4147-afff-3cff3a5812a1",
   "metadata": {},
   "source": [
    "# Augmentation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9183194-190b-430d-94e2-721f1ffaffa9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6293f3be-64ec-4f64-87ef-36812f429626",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "path_images = '/home/ld_brito/DeepL/dataset description/train/images'\n",
    "path_labels = '/home/ld_brito/DeepL/dataset description/train/labels'\n",
    "list_lables = os.listdir(path_labels)\n",
    "path_to_save_imgs = '/home/ld_brito/DeepL/dataset description/train_augmented/images'\n",
    "path_to_save_labels = '/home/ld_brito/DeepL/dataset description/train_augmented/labels'\n",
    "\n",
    "p = 0.5 #probability that an image has to be aumgented\n",
    "\n",
    "for img in os.listdir(path_images):\n",
    "\n",
    "    img_file = path_images + '/' + img\n",
    "    txt_file = img.split('.')[0]+'.txt'\n",
    "    in_labels = txt_file in list_lables\n",
    "    \n",
    "    prob_hFlip = random.uniform(0, 1)\n",
    "    if prob_hFlip <= p:\n",
    "        horizontalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip = random.uniform(0, 1)\n",
    "    if prob_vFlip <= p:\n",
    "        verticalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_dFlip = random.uniform(0, 1)\n",
    "    if prob_dFlip <= p:\n",
    "        doubleFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_hue = random.uniform(0, 1)\n",
    "    if prob_hue <= p:\n",
    "        raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_contrst = random.uniform(0, 1)\n",
    "    if prob_contrst <= p:\n",
    "        adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hFlip_hue = random.uniform(0, 1)\n",
    "    if prob_hFlip_hue <= p:\n",
    "        horizontalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip_hue = random.uniform(0, 1)\n",
    "    if prob_vFlip_hue <= p:\n",
    "        verticalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_hue = random.uniform(0, 1)\n",
    "    if prob_dFlip_hue <= p:\n",
    "        doubleFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_hFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_hFlip_contrst <= p:\n",
    "        horizontalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_vFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_vFlip_contrst <= p:\n",
    "        verticalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_dFlip_contrst <= p:\n",
    "        doubleFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_hue_contrst <= p:\n",
    "        raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_hFlip_hue_contrst <= p:\n",
    "        horizontalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_vFlip_hue_contrst <= p:\n",
    "        verticalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_dFlip_hue_contrst <= p:\n",
    "        doubleFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    # copy original files to the augmented directory\n",
    "    shutil.copy(img_file,path_to_save_imgs)\n",
    "    if in_labels == True:\n",
    "        shutil.copy(path_labels + '/' + txt_file,path_to_save_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340efea-c540-4e54-abd7-36b730348495",
   "metadata": {},
   "source": [
    "## Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58056722-f29f-43b4-bc99-bbac178ca372",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "path_images = '/home/ld_brito/DeepL/dataset description/valid/images'\n",
    "path_labels = '/home/ld_brito/DeepL/dataset description/valid/labels'\n",
    "list_lables = os.listdir(path_labels)\n",
    "path_to_save_imgs = '/home/ld_brito/DeepL/dataset description/valid_augmented/images'\n",
    "path_to_save_labels = '/home/ld_brito/DeepL/dataset description/valid_augmented/labels'\n",
    "\n",
    "p = 0.5 #probability that an image has to be aumgented\n",
    "\n",
    "for img in os.listdir(path_images):\n",
    "\n",
    "    img_file = path_images + '/' + img\n",
    "    txt_file = img.split('.')[0]+'.txt'\n",
    "    in_labels = txt_file in list_lables\n",
    "    \n",
    "    prob_hFlip = random.uniform(0, 1)\n",
    "    if prob_hFlip <= p:\n",
    "        horizontalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip = random.uniform(0, 1)\n",
    "    if prob_vFlip <= p:\n",
    "        verticalFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_dFlip = random.uniform(0, 1)\n",
    "    if prob_dFlip <= p:\n",
    "        doubleFlip(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_hue = random.uniform(0, 1)\n",
    "    if prob_hue <= p:\n",
    "        raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_contrst = random.uniform(0, 1)\n",
    "    if prob_contrst <= p:\n",
    "        adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hFlip_hue = random.uniform(0, 1)\n",
    "    if prob_hFlip_hue <= p:\n",
    "        horizontalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip_hue = random.uniform(0, 1)\n",
    "    if prob_vFlip_hue <= p:\n",
    "        verticalFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_hue = random.uniform(0, 1)\n",
    "    if prob_dFlip_hue <= p:\n",
    "        doubleFlip_raiseHue(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_hFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_hFlip_contrst <= p:\n",
    "        horizontalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_vFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_vFlip_contrst <= p:\n",
    "        verticalFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_contrst = random.uniform(0, 1)\n",
    "    if prob_dFlip_contrst <= p:\n",
    "        doubleFlip_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_hue_contrst <= p:\n",
    "        raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_hFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_hFlip_hue_contrst <= p:\n",
    "        horizontalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    prob_vFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_vFlip_hue_contrst <= p:\n",
    "        verticalFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "        \n",
    "    prob_dFlip_hue_contrst = random.uniform(0, 1)\n",
    "    if prob_dFlip_hue_contrst <= p:\n",
    "        doubleFlip_raiseHue_adjustContrast(img_file,txt_file,path_labels,in_labels,path_to_save_imgs,path_to_save_labels)\n",
    "    \n",
    "    # copy original files to the augmented directory\n",
    "    shutil.copy(img_file,path_to_save_imgs)\n",
    "    if in_labels == True:\n",
    "        shutil.copy(path_labels + '/' + txt_file,path_to_save_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
